{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax.nn import sigmoid\n",
    "import sys\n",
    "import jax.random as random\n",
    "sys.path.append(\"/Users/changjc/workspace/bayesianquilts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abilities = np.array([0, 0.5, 0.25])[:, np.newaxis, np.newaxis]\n",
    "difficulties = np.array([[0, 1, 2, 3], [-2, 0, 3, 4]])[np.newaxis, ...]\n",
    "discriminations = np.array([1, 2])[np.newaxis, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "ddsigmoid = lambda x: sigmoid(x) * (1 - sigmoid(x)) * (1 - 2 * sigmoid(x))\n",
    "\n",
    "\n",
    "#\n",
    "# N x I x K\n",
    "\n",
    "\n",
    "def p_ni(abilities, difficulties, discriminations):  # dimensio\n",
    "    N = abilities.shape[0]\n",
    "    I = discriminations.shape[1]\n",
    "    K = difficulties.shape[-1] + 1\n",
    "    p_cum = sigmoid(discriminations * (abilities - difficulties))\n",
    "    # first partials, will be N x I x K x d where d is the dimension of the parameter\n",
    "    # d_cum_abilites will be N x I x K x N\n",
    "    dp_cum_dabilities = (\n",
    "        (p_cum * (1 - p_cum))[..., jnp.newaxis]\n",
    "        * jnp.ones((N, I, K - 1, N))\n",
    "        * discriminations[..., jnp.newaxis]\n",
    "    )\n",
    "    dp_cum_ddifficulties = (\n",
    "        -(p_cum * (1 - p_cum))[..., jnp.newaxis, jnp.newaxis]\n",
    "        * jnp.ones((N, I, K - 1, I, K - 1))\n",
    "        * discriminations[..., np.newaxis, jnp.newaxis]\n",
    "    )\n",
    "    dp_cum_ddiscrimintations = (\n",
    "        (p_cum * (1 - p_cum))[..., np.newaxis]\n",
    "        * jnp.ones((N, I, K - 1, I))\n",
    "        * ((abilities - difficulties)[..., jnp.newaxis])\n",
    "    )\n",
    "\n",
    "    # second partials\n",
    "    # Diagonal terms\n",
    "    d2p_cum_dabilities2 = (\n",
    "        (p_cum * (1 - p_cum) * (1 - 2 * p_cum))[..., jnp.newaxis]\n",
    "        * jnp.ones((N, I, K - 1, N))\n",
    "        * discriminations[..., jnp.newaxis] ** 2\n",
    "    )\n",
    "    d2p_cum_ddiscrimintations2 = (\n",
    "        (p_cum * (1 - p_cum) * (1 - 2 * p_cum))[..., jnp.newaxis]\n",
    "        * jnp.ones((N, I, K - 1, I))\n",
    "        * (abilities - difficulties)[..., jnp.newaxis] ** 2\n",
    "    )\n",
    "    d2p_cum_ddifficulties2 = (\n",
    "        p_cum * (1 - p_cum) * (1 - 2 * p_cum) * discriminations * discriminations**2\n",
    "    )[..., jnp.newaxis, jnp.newaxis] * jnp.ones((N, I, K - 1, I, K - 1))\n",
    "\n",
    "    # mixed partials\n",
    "    d2p_cum_dabilities_difficulties = (\n",
    "        -(p_cum * (1 - p_cum) * (1 - 2 * p_cum))[\n",
    "            ..., jnp.newaxis, jnp.newaxis, jnp.newaxis\n",
    "        ]\n",
    "        * discriminations[..., jnp.newaxis, jnp.newaxis, jnp.newaxis] ** 2\n",
    "    ) * jnp.ones((N, I, K - 1, N, I, K - 1))\n",
    "    d2p_cum_dabilities_discriminations = (\n",
    "        (p_cum * (1 - p_cum))\n",
    "        + p_cum * (1 - p_cum) * (1 - 2 * p_cum) * discriminations**2\n",
    "    )[..., jnp.newaxis, jnp.newaxis] * jnp.ones((N, I, K - 1, N, I))\n",
    "    d2p_cum_ddifficulties_discriminations = (\n",
    "        -p_cum * (1 - p_cum)\n",
    "        - p_cum\n",
    "        * (1 - p_cum)\n",
    "        * (1 - 2 * p_cum)\n",
    "        * discriminations\n",
    "        * (abilities - difficulties)\n",
    "    )[..., jnp.newaxis, jnp.newaxis, jnp.newaxis] * jnp.ones((N, I, K - 1, I, I, K - 1))\n",
    "\n",
    "    p_cum = jnp.pad(p_cum, ((0, 0), (0, 0), (1, 0)), constant_values=0)\n",
    "    p_cum = jnp.pad(p_cum, ((0, 0), (0, 0), (0, 1)), constant_values=1)\n",
    "\n",
    "    # padding for gradient\n",
    "    dp_cum_dabilities = jnp.pad(\n",
    "        dp_cum_dabilities, ((0, 0), (0, 0), (1, 1), (0, 0)), constant_values=0\n",
    "    )\n",
    "    dp_cum_ddifficulties = jnp.pad(\n",
    "        dp_cum_ddifficulties,\n",
    "        ((0, 0), (0, 0), (1, 1), (0, 0), (0, 0)),\n",
    "        constant_values=0,\n",
    "    )\n",
    "    dp_cum_ddiscrimintations = jnp.pad(\n",
    "        dp_cum_ddiscrimintations, ((0, 0), (0, 0), (1, 1), (0, 0)), constant_values=0\n",
    "    )\n",
    "    # diagonal hessian\n",
    "    d2p_cum_dabilities2 = jnp.pad(\n",
    "        d2p_cum_dabilities2, ((0, 0), (0, 0), (1, 1), (0, 0)), constant_values=0\n",
    "    )\n",
    "    d2p_cum_ddifficulties2 = jnp.pad(\n",
    "        d2p_cum_ddifficulties2,\n",
    "        ((0, 0), (0, 0), (1, 1), (0, 0), (0, 0)),\n",
    "        constant_values=0,\n",
    "    )\n",
    "    d2p_cum_ddiscrimintations2 = jnp.pad(\n",
    "        d2p_cum_ddiscrimintations2, ((0, 0), (0, 0), (1, 1), (0, 0)), constant_values=0\n",
    "    )\n",
    "    # pad the mixed terms\n",
    "    d2p_cum_dabilities_difficulties = jnp.pad(\n",
    "        d2p_cum_dabilities_difficulties,\n",
    "        ((0, 0), (0, 0), (1, 1), (0, 0), (0, 0), (0, 0)),\n",
    "        constant_values=0,\n",
    "    )\n",
    "    d2p_cum_ddifficulties_discriminations = jnp.pad(\n",
    "        d2p_cum_ddifficulties_discriminations,\n",
    "        ((0, 0), (0, 0), (1, 1), (0, 0), (0, 0), (0, 0)),\n",
    "        constant_values=0,\n",
    "    )\n",
    "    d2p_cum_dabilities_discriminations = jnp.pad(\n",
    "        d2p_cum_dabilities_discriminations,\n",
    "        ((0, 0), (0, 0), (1, 1), (0, 0), (0, 0)),\n",
    "        constant_values=0,\n",
    "    )\n",
    "\n",
    "    dp_dabilities = dp_cum_dabilities[:, :, 1:, ...] - dp_cum_dabilities[:, :, :-1, ...]\n",
    "    dp_ddifficulties = (\n",
    "        dp_cum_ddifficulties[:, :, 1:, ...] - dp_cum_ddifficulties[:, :, :-1, ...]\n",
    "    )\n",
    "    dp_ddiscrimintations = (\n",
    "        dp_cum_ddiscrimintations[:, :, 1:, ...]\n",
    "        - dp_cum_ddiscrimintations[:, :, :-1, ...]\n",
    "    )\n",
    "\n",
    "    d2p_dabilities = (\n",
    "        d2p_cum_dabilities2[:, :, 1:, ...] - d2p_cum_dabilities2[:, :, :-1, ...]\n",
    "    )\n",
    "    d2p_ddiscriminations = (\n",
    "        d2p_cum_ddiscrimintations2[:, :, 1:, ...]\n",
    "        - d2p_cum_ddiscrimintations2[:, :, :-1, ...]\n",
    "    )\n",
    "    d2p_ddifficulties = (\n",
    "        d2p_cum_ddifficulties2[:, :, 1:, ...] - d2p_cum_ddifficulties2[:, :, :-1, ...]\n",
    "    )\n",
    "    d2p_dabilities_ddifficulties = (\n",
    "        d2p_cum_dabilities_difficulties[:, :, 1:, ...]\n",
    "        - d2p_cum_dabilities_difficulties[:, :, :-1, ...]\n",
    "    )\n",
    "    d2p_dabilities_ddiscrimintations = (\n",
    "        d2p_cum_dabilities_discriminations[:, :, 1:, ...]\n",
    "        - d2p_cum_dabilities_discriminations[:, :, :-1, ...]\n",
    "    )\n",
    "\n",
    "    d2p_ddifficulties_ddiscrimintations = (\n",
    "        d2p_cum_ddifficulties_discriminations[:, :, 1:, ...]\n",
    "        - d2p_cum_ddifficulties_discriminations[:, :, :-1, ...]\n",
    "    )\n",
    "\n",
    "    p = p_cum[..., 1:] - p_cum[..., :-1]\n",
    "\n",
    "    # compute derivatives\n",
    "\n",
    "    gradients = {\n",
    "        \"abilities\": dp_dabilities,\n",
    "        \"difficulties\": dp_ddifficulties,\n",
    "        \"discriminations\": dp_ddiscrimintations,\n",
    "    }\n",
    "\n",
    "    grad_log_p = {\n",
    "        \"abilities\": dp_dabilities / p_cum[..., jnp.newaxis],\n",
    "        \"difficulties\": dp_ddifficulties / p_cum[..., jnp.newaxis, jnp.newaxis],\n",
    "        \"discriminations\": dp_ddiscrimintations / p_cum[..., jnp.newaxis],\n",
    "    }\n",
    "    grad2_p = {\n",
    "        (\"abilities\", \"abilites\"): d2p_dabilities,\n",
    "        (\"discriminations\", \"discriminations\"): d2p_ddiscriminations,\n",
    "        (\"difficulties\", \"difficulties\"): d2p_ddifficulties,\n",
    "    }\n",
    "    grad2_log_p = {}\n",
    "\n",
    "    return {\n",
    "        \"p\": p,\n",
    "        \"log(p)\": jnp.log(p),\n",
    "        \"grad(p)\": gradients,\n",
    "        \"grad(log(p))\": grad_log_p,\n",
    "        \"grad(grad(p))\": grad2_p,\n",
    "    }\n",
    "\n",
    "\n",
    "def find_a(abilities, difficulties, discriminations):\n",
    "    vals = p_ni(abilities, difficulties, discriminations)\n",
    "\n",
    "\n",
    "def find_b(abilities, difficulties, discriminations, a):\n",
    "    return\n",
    "\n",
    "\n",
    "p_ni(abilities, difficulties, discriminations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.io'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoencirt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrwa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m item_text, get_data, to_reverse\n",
      "File \u001b[0;32m~/workspace/autoencirt/autoencirt/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mirt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/workspace/autoencirt/autoencirt/irt/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mirt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IRTModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GRModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactorizedgrm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactorizedGRModel\n",
      "File \u001b[0;32m~/workspace/autoencirt/autoencirt/irt/irt.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesianquilts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesianquilts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubstrates\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions \u001b[38;5;28;01mas\u001b[39;00m tfd\n",
      "File \u001b[0;32m~/workspace/bayesianquilts/bayesianquilts/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogistic_bayesianquilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticBayesianquilt\n",
      "File \u001b[0;32m~/workspace/bayesianquilts/bayesianquilts/model.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesianquilts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactorizedDistributionMoments\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesianquilts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_loop\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbayesianquilts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminibatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minibatch_fit_surrogate_posterior\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBayesianModel\u001b[39;00m(ABC, nnx\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/workspace/bayesianquilts/bayesianquilts/util.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unfreeze\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoints\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubstrates\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2jax \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/workspace/autoencirt/env/lib/python3.11/site-packages/flax/training/checkpoints.py:45\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_serialization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m   GlobalAsyncCheckpointManager,\n\u001b[1;32m     41\u001b[0m   get_tensorstore_spec,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultihost_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_global_devices\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, core, errors, io, serialization, traverse_util\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m orbax_utils\n\u001b[1;32m     49\u001b[0m _READ_CHECKPOINT_EVENT: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/jax/checkpoint/read/durations_sec\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/workspace/autoencirt/env/lib/python3.11/site-packages/flax/io.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m gfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     44\u001b[0m   io_mode \u001b[38;5;241m=\u001b[39m BackendMode\u001b[38;5;241m.\u001b[39mTF\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.io'"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "from autoencirt.data.rwa import item_text, get_data, to_reverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = get_data(reorient=True, pandas=True)\n",
    "X = pd_data[0].iloc[:, :22]\n",
    "N, I = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
